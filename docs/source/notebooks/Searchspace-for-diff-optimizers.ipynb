{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search space for different types of optimizer and schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different optimizers have different update rules and behavior, and they may perform better or worse depending on the specific dataset and model architecture. \n",
    "Hence, trying out different optimizers and learning rate schedulers can be a good technique for HPO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Providing the name of the optimizer in the config will create an object of the associated optimizer class. Hence, simply changing the name in search space will result in a mismatch in the class type, causing an error.\n",
    "\n",
    "* To work with different optimizers effectively in the ablator, it is necessary to create custom OptimizerConfig objects that can handle passing either torch or custom optimizers to the ablator.\n",
    "\n",
    "* This is similar with scheduler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For different optimizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````make_optimizer```` function creates an optimizer object based on inputs from the custom configs.\n",
    "\n",
    "This example supports three optimizers: Adam, AdamW, and SGD, however, we can also pass our own custom defined optimizers.\n",
    "\n",
    "* Creates a list of model parameters called parameter_groups.\n",
    "* Defines dictionaries with specific configuration settings for each optimizer.\n",
    "* Sets optimizer parameters using the defined dictionaries, learning rate, and parameter groups.\n",
    "\n",
    "Returns the optimizer object."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def make_optimizer(optimizer_name: str, model: nn.Module, lr: float):\n",
    "\n",
    "    parameter_groups = [v for k, v in model.named_parameters()]\n",
    "\n",
    "    adamw_parameters = {\n",
    "      \"betas\": (0.0, 0.1),\n",
    "      \"eps\": 0.001,\n",
    "      \"weight_decay\": 0.1\n",
    "    }\n",
    "    adam_parameters = {\n",
    "      \"betas\" : (0.0, 0.1),\n",
    "      \"weight_decay\": 0.0\n",
    "    }\n",
    "\n",
    "    sgd_parameters = {\n",
    "      \"momentum\": 0.9,\n",
    "      \"weight_decay\": 0.1\n",
    "    }\n",
    "\n",
    "    Optimizer = {\n",
    "        \"adam\": optim.Adam(parameter_groups, lr = lr, **adam_parameters),\n",
    "        \"adamw\": optim.AdamW(parameter_groups, lr = lr, **adamw_parameters),\n",
    "        \"sgd\": optim.SGD(parameter_groups, lr = lr, **sgd_parameters),\n",
    "    }[optimizer_name]\n",
    "\n",
    "\n",
    "    return Optimizer\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create own ````CustomOptimizerConfig````.\n",
    "\n",
    "Since, we are creating a Optimizer configuration. Ablator requires a method: make_optimizer with input as model. Thus, creating the method and returning the torch optimizer from our previous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@configclass\n",
    "class CustomOptimizerConfig(ConfigBase):\n",
    "    name: Literal[\"adam\", \"adamw\", \"sgd\"] = \"adam\"\n",
    "    lr: float = 0.001\n",
    "\n",
    "    def make_optimizer(self, model: torch.nn.Module):\n",
    "        return make_optimizer(self.name, model, self.lr)\n",
    "\n",
    "optimizer_config = CustomOptimizerConfig(name = \"adam\", lr = 0.001)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For different Schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function ````make_scheduler```` which takes all the parameters and passes to its respective learning rate schedulers for torch.\n",
    "\n",
    "Returns the torch scheduler object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau, StepLR\n",
    "\n",
    "def make_scheduler(scheduler: str,model: nn.Module, optimizer: torch.optim):\n",
    "\n",
    "  step_parameters = {\n",
    "      \"step_size\" : 1,\n",
    "      \"gamma\" : 0.99\n",
    "  }\n",
    "\n",
    "  plateau_parameters = {\n",
    "    \"patience\":  10,\n",
    "    \"min_lr\":  1e-5,\n",
    "    \"mode\":  \"min\",\n",
    "    \"factor\":   0.0,\n",
    "    \"threshold\":  1e-4 \n",
    "  }\n",
    "\n",
    "  cycle_parameters = {\n",
    "      \"max_lr\": 1e-3,\n",
    "     \"total_steps\": 10\n",
    "  }\n",
    "  \n",
    "  Scheduler = {\n",
    "      \"step\": StepLR(optimizer, **step_parameters),\n",
    "      \"cycle\": OneCycleLR(optimizer, **cycle_parameters),\n",
    "      \"plateau\": ReduceLROnPlateau(optimizer, **plateau_parameters),\n",
    "    }[scheduler]\n",
    "\n",
    "  return Scheduler\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, creating a custom ````CustomSchedulerConfig```` and making the required method ````make_scheduler```` with the inputs: model and optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@configclass\n",
    "class CustomSchedulerConfig(ConfigBase):\n",
    "    name: Literal[\"step\", \"cycle\", \"plateau\"] = \"step\"\n",
    "\n",
    "    def make_scheduler(self, model: torch.nn.Module, optimizer: torch.optim):\n",
    "        return make_scheduler(self.name, model, optimizer)\n",
    "\n",
    "scheduler_config = CustomSchedulerConfig(name = \"step\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````CustomTrainConfig```` takes both object to define the train configuration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@configclass\n",
    "class CustomTrainConfig(TrainConfig):\n",
    "  optimizer_config: CustomOptimizerConfig\n",
    "  scheduler_config: CustomSchedulerConfig\n",
    "\n",
    "\n",
    "train_config = CustomTrainConfig(\n",
    "    dataset=\"[Your Dataset]\",\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    optimizer_config = optimizer_config,\n",
    "    scheduler_config = scheduler_config,\n",
    "    rand_weights_init = True\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try out different optimizers and schedulers by providing a search space to ablator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_space_for_optimizers = {\n",
    "    ...\n",
    "    \"train_config.optimizer_config.name\": SearchSpace(categorical_values= [\"adam\", \"sgd\", \"adamw\"]),\n",
    "    ...\n",
    "}\n",
    "\n",
    "search_space_for_schedulers = {\n",
    "    ...\n",
    "    \"train_config.scheduler_config.name\": SearchSpace(categorical_values= [\"step\", \"cycle\", \"plateau\"]),\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Finally, with this, we can now test different optimizers and schedulers for our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
